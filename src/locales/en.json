{
    "title":"Portfolio",
    "hello": "Hello! My name is Diego",
    "resume": "I am a Software Development and Management Engineer. I graduated in 2022, but I have been immersed in the professional field since May 2020. Throughout my career, I have worked in various areas of my field, from QA tester to taking on developer roles, and currently serving as a Tech Leader.",
    "changeLanguage": "Change language",
    "about": "About",
    "skills":"Skills",
    "contact":"Contact",
    "projects":"Projects",
    "experience":"Experience",
    "introduction":"Introduction",
    "overview":"Overview.",
    "overviewText":"I began my professional development in May 2020 at Exos Technology, where I had the opportunity to work as an intern in the QA testing area. During this time, I acquired solid skills in unit and integration testing of software projects, as well as mastering comprehensive documentation and evidence file generation. My most notable contribution in QA was the implementation of test automation tools, such as SmartBear, Katalon, and Selenium. These tools were essential for automating software regression testing, allowing us to generate multiple scenarios and a comprehensive results matrix. This approach not only increased the efficiency of our development process, but also significantly improved the quality and consistency of the delivered software. Later on, I transitioned to the development area, where I played a key role in various projects and faced diverse challenges. At Exos, a company with a particular focus on the financial sector, I led the implementation of software solutions of various kinds; from creating simple microservices for dispute and clarification management, to developing sophisticated algorithms and implementing complex batch processes; I always actively participated in all aspects of development. My progression within the company led me to take on the role of technical leader, where, in addition to continuing to develop innovative solutions, I dedicated myself to planning (work plan generation), execution (progress review, testing), and successful delivery of all projects.",
    "frontend": "Frontend Developer",
    "backend": "Backend Developer",
    "ux": "UX/UI Design",
    "qa": "QA",
    "mySkills": "My skills",
    "techs": "Technologies.",
    "soFar": "What I've done so far...",
    "workExperience": "Work Experience.",
    "myResume": "My resume",
    "touch": "Get in touch",
    "contactTitle": "Contact.",
    "contactInfo": "Reach me",
    "yourName": "Your Name",
    "yourEmail": "Your Email",
    "yourMsg": "Your Message",
    "send": "Send",
    "sendingMsg": "Thank you. I will get back to you as soon as possible.",
    "errorMsg":"Something went wrong. Please try again.",
    "mail": "Email",
    "phone": "Phone",
    "caseStudies": "Case studies",
    "proyects": "Proyects.",
    "achievements":"Explore a selection of projects where I have acquired and put into practice new skills, enriching my experience to achieve the set goals. These projects not only reflect the work done but also the tangible impact I have achieved. Explore how my solutions have generated measurable results and have contributed to the success of both my clients and the involved teams.",
    "saleCapture": "Sales capture BBVA-AMEX",
    "saleCaptureDescription": "Undoubtedly, one of the most challenging projects I have had the opportunity to develop. Not only because of its technical and logical complexity but also due to the need to process large volumes of data. This module is part of a larger project called eBind, being the first to implement GraphQL, Apollo Federation, and Spring Boot. The decision to use GraphQL was driven by the need for a centralized service that would allow different modules and processes to access the services implemented for this project. Since sales are interconnected with other processes, the flexibility offered by the Netflix library was essential. With this independent GraphQL service core, an API Gateway with Apollo Federation was created to facilitate requests to the Sales service. Throughout the development process, different approaches had to be worked on, as the goal was to process 16k records in 3 minutes, which was ultimately achieved.",
    "kafkaMigracion": "Kafka migration",
    "kafkaMigracionDescription": "Throughout my career, I have worked on complex data processing projects, designing Batch solutions to handle large transaction volumes. However, when facing increased data growth and real-time processing requirements, I implemented a Kafka-based architecture that enabled immediate processing of up to 1,000 transactions per second. Previously, our Batch approach processed around 20 million daily transactions but only overnight, limiting the client’s capacity. With Kafka, CDC, and Oracle GoldenGate, I configured an environment that enabled continuous real-time database connections, including Informix and Oracle, transforming our processes to real-time. In an additional project, I designed and implemented a solution using JPOS as a producer to receive ISO8583 transactions, transform them, and send them as byte arrays to a Kafka cluster. This architecture allowed the cluster to receive transactions from multiple banks, facilitating transaction propagation through a central consumer that routed transactions to specific microservices for each bank. I used Spring Boot to configure brokers and topics, ensuring scalability and efficiency in a clustered environment.",
    "etl": "Kafka migration",
    "etlDesc": "Throughout my career, I have worked on complex data processing projects, designing Batch solutions to handle large transaction volumes. However, when facing increased data growth and real-time processing requirements, I implemented a Kafka-based architecture that enabled immediate processing of up to 1,000 transactions per second. Previously, our Batch approach processed around 20 million daily transactions but only overnight, limiting the client’s capacity. With Kafka, CDC, and Oracle GoldenGate, I configured an environment that enabled continuous real-time database connections, including Informix and Oracle, transforming our processes to real-time. In an additional project, I designed and implemented a solution using JPOS as a producer to receive ISO8583 transactions, transform them, and send them as byte arrays to a Kafka cluster. This architecture allowed the cluster to receive transactions from multiple banks, facilitating transaction propagation through a central consumer that routed transactions to specific microservices for each bank. I used Spring Boot to configure brokers and topics, ensuring scalability and efficiency in a clustered environment.",

    "downloadResume": "Download my resume"
}
